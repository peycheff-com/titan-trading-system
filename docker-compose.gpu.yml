# Docker Compose for Self-Hosted AI (Alternative to deploy-gpu.sh)
# Use this on a pre-existing GPU server with Docker + NVIDIA Container Toolkit
#
# Prerequisites:
#   1. NVIDIA drivers installed
#   2. Docker with nvidia-container-toolkit
#   3. Verify: docker run --rm --gpus all nvidia/cuda:12.1-base nvidia-smi

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: titan-vllm
    runtime: nvidia
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_HUB_ENABLE_HF_TRANSFER=1
    command: >
      --model moonshotai/Kimi-K2.5-Instruct
      --tensor-parallel-size 1
      --quantization int4
      --max-model-len 131072
      --port 8000
      --host 0.0.0.0
    ports:
      - '8000:8000'
    volumes:
      - vllm-cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8000/health']
      interval: 30s
      timeout: 10s
      retries: 3

  nginx:
    image: nginx:alpine
    container_name: titan-ai-proxy
    ports:
      - '80:80'
      - '443:443'
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
      # - ./certs:/etc/ssl/titan:ro  # Uncomment for TLS
    depends_on:
      - vllm
    restart: unless-stopped

volumes:
  vllm-cache:
    driver: local
